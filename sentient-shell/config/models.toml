# SentientOS Model Configuration
# Defines available LLM models, their capabilities, and routing preferences

[models.phi2_local]
name = "Microsoft Phi-2"
provider = "local"
location = "boot"
capabilities = [
    "tool_calling",
    "basic_reasoning",
    "command_interpretation",
    "system_diagnostics"
]
performance_tier = "fast"
context_length = 2048
priority = 100  # Higher priority for local/offline scenarios
use_cases = [
    "tool_interpretation",
    "command_parsing",
    "quick_responses",
    "offline_operation"
]

[models.llama3_local]
name = "Llama 3 8B"
provider = "ollama"
endpoint = "http://localhost:11434"
model_id = "llama3:8b"
capabilities = [
    "general_reasoning",
    "code_generation",
    "conversation",
    "tool_calling"
]
performance_tier = "balanced"
context_length = 8192
priority = 80
use_cases = [
    "general_qa",
    "code_analysis",
    "documentation",
    "moderate_complexity"
]

[models.deepseek_v2]
name = "DeepSeek V2 Coder"
provider = "ollama"
endpoint = "http://192.168.69.197:11434"
model_id = "deepseek-coder-v2"
capabilities = [
    "advanced_reasoning",
    "code_generation",
    "complex_analysis",
    "long_context",
    "tool_orchestration"
]
performance_tier = "powerful"
context_length = 32768
priority = 60  # Lower priority, use for complex tasks
use_cases = [
    "complex_reasoning",
    "code_writing",
    "system_design",
    "multi_step_planning"
]

[models.mistral_instruct]
name = "Mistral 7B Instruct"
provider = "ollama"
endpoint = "http://192.168.69.197:11434"
model_id = "mistral:7b-instruct"
capabilities = [
    "instruction_following",
    "tool_calling",
    "reasoning",
    "fast_inference"
]
performance_tier = "fast"
context_length = 8192
priority = 70
use_cases = [
    "command_execution",
    "tool_orchestration",
    "quick_analysis"
]

[models.llama3_vision]
name = "Llama 3.2 Vision"
provider = "ollama"
endpoint = "http://192.168.69.197:11434"
model_id = "llama3.2-vision"
capabilities = [
    "vision_understanding",
    "screenshot_analysis",
    "ui_interaction",
    "visual_debugging"
]
performance_tier = "specialized"
context_length = 4096
priority = 50
use_cases = [
    "screenshot_analysis",
    "visual_debugging",
    "ui_automation"
]

# Routing rules define how to select models based on intent
[routing]

# Default model for general use
default_model = "phi2_local"

# Offline fallback chain
offline_chain = ["phi2_local", "llama3_local"]

# Intent-based routing
[routing.intents]
tool_call = ["phi2_local", "mistral_instruct", "llama3_local"]
code_generation = ["deepseek_v2", "llama3_local"]
system_analysis = ["deepseek_v2", "llama3_local", "mistral_instruct"]
quick_response = ["phi2_local", "mistral_instruct"]
visual_analysis = ["llama3_vision"]
complex_reasoning = ["deepseek_v2", "llama3_local"]

# Performance tiers (latency requirements)
[routing.performance]
realtime = ["phi2_local", "mistral_instruct"]  # <100ms
fast = ["llama3_local", "mistral_instruct"]     # <500ms
balanced = ["llama3_local", "deepseek_v2"]      # <2s
powerful = ["deepseek_v2"]                       # No limit

# Context length routing
[routing.context]
short = ["phi2_local", "mistral_instruct"]      # <2K tokens
medium = ["llama3_local", "mistral_instruct"]   # <8K tokens
long = ["deepseek_v2"]                           # <32K tokens

# Load balancing configuration
[load_balancing]
strategy = "capability_first"  # Options: round_robin, capability_first, latency_based
max_concurrent_requests = 3
timeout_ms = 30000
retry_attempts = 2